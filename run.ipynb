{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "\n",
    "# from test_utils import test\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run de A a Z - Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading  data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.genfromtxt(\"dataset/x_test.csv\", delimiter=\",\", dtype = str, max_rows=1)\n",
    "labels = np.delete(labels,0) # delete the label 'id' as we dont have it in x_train and x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the high number of features with all different exceptions values and linearity, we decided to select manually the features we found relevant. The list below is the name of the feature, the exception values that we have to remplace, and wheter we remplace by majority (True) or by the mean (False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [(\"MSCODE\", [], True),\n",
    "            (\"_HCVU651\", [9], True),\n",
    "            (\"_RFHYPE5\", [9], True),\n",
    "            (\"_RFCHOL\", [9], True),\n",
    "            (\"_RACE\",[9], True),\n",
    "            (\"_BMI5\",[], False),\n",
    "            (\"_EDUCAG\",[9], True),\n",
    "            (\"_INCOMG\",[9], True),\n",
    "            (\"_SMOKER3\",[9], True),\n",
    "            (\"_DRNKWEK\",[99900], True),\n",
    "            (\"_FRUTSUM\",[], True),\n",
    "            (\"_VEGESUM\",[], True),\n",
    "            (\"PA1MIN_\",[], True),\n",
    "            (\"GENHLTH\",[7,9], False),\n",
    "            (\"CHECKUP1\",[7,9], False),\n",
    "            (\"MENTHLTH\",[88, 77, 99], False),\n",
    "            (\"BPHIGH4\",[7,9], True),\n",
    "            (\"BPMEDS\",[7,9], True),\n",
    "            (\"TOLDHI2\",[7,9], True),\n",
    "            (\"CHCOCNCR\",[7,8,9], True),\n",
    "            (\"DIABETE3\",[7,8,9], True),\n",
    "            (\"SEX\",[], True),\n",
    "            (\"QLACTLM2\",[7,9], True),\n",
    "            (\"AVEDRNK2\",[77, 99], False),\n",
    "            (\"EXERANY2\",[7,9], True),\n",
    "            (\"SHINGLE2\", [7,9], True),\n",
    "            (\"LMTJOIN3\", [7,9], True),\n",
    "            (\"CVDASPRN\", [7,9], True)\n",
    "            ]\n",
    "cleaned_x_train = clean_data(labels, label_list, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning in another way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7  # You want to drop columns with > 50% NaNs\n",
    "column_count = x_train.shape[0]\n",
    "max_nan_count = threshold * column_count\n",
    "\n",
    "# Create a mask to identify columns with too many NaNs\n",
    "nan_mask = np.sum(np.isnan(x_train), axis=0) <= max_nan_count\n",
    "\n",
    "# Use the mask to select the columns with fewer NaNs\n",
    "x_train_filtered = x_train[:, nan_mask]\n",
    "x_test_filtered = x_test[:, nan_mask]\n",
    "\n",
    "# drop the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_filtered[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.728971962616825\n",
      "46.728971962616825\n",
      "42.05607476635514\n",
      "32.398753894081\n",
      "38.940809968847354\n",
      "41.43302180685358\n",
      "44.54828660436137\n",
      "41.74454828660436\n",
      "41.74454828660436\n",
      "44.85981308411215\n",
      "50.155763239875384\n",
      "42.99065420560748\n",
      "46.10591900311526\n",
      "42.679127725856695\n",
      "36.7601246105919\n",
      "38.006230529595015\n",
      "43.925233644859816\n",
      "42.05607476635514\n",
      "50.77881619937694\n",
      "38.31775700934579\n",
      "39.56386292834891\n",
      "45.17133956386293\n",
      "30.8411214953271\n",
      "42.679127725856695\n",
      "42.36760124610592\n",
      "47.35202492211838\n",
      "52.024922118380054\n",
      "47.66355140186916\n"
     ]
    }
   ],
   "source": [
    "# want to see which labels are way to small\n",
    "\n",
    "for label in label_list:\n",
    "    l, _, _ = label\n",
    "    id = np.where(labels == l)[0][0]\n",
    "    x = x_train[:][id]\n",
    "\n",
    "    # Count the number of NaN values\n",
    "    nans= np.count_nonzero(np.isnan(x))\n",
    "    nan_perc = nans/len(x) *100\n",
    "    print(nan_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/Julie/Desktop/EPFL/MA3/ML/ML_project1/run.ipynb Cellule 17\u001b[0m line \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Julie/Desktop/EPFL/MA3/ML/ML_project1/run.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m degree \u001b[39m=\u001b[39m \u001b[39m25\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Julie/Desktop/EPFL/MA3/ML/ML_project1/run.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m## tx is cleaned data \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Julie/Desktop/EPFL/MA3/ML/ML_project1/run.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m poly \u001b[39m=\u001b[39m build_poly(x_train_filtered,degree)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Julie/Desktop/EPFL/MA3/ML/ML_project1/run.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m w , loss \u001b[39m=\u001b[39m ridge_regression(y_train,poly,lambda_)\n",
      "File \u001b[0;32m~/Desktop/EPFL/MA3/ML/ML_project1/helpers.py:194\u001b[0m, in \u001b[0;36mbuild_poly\u001b[0;34m(x, degree)\u001b[0m\n\u001b[1;32m    192\u001b[0m poly \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39mlen\u001b[39m(x), \u001b[39m1\u001b[39m))\n\u001b[1;32m    193\u001b[0m \u001b[39mfor\u001b[39;00m deg \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, degree\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 194\u001b[0m     poly \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mc_[poly, np\u001b[39m.\u001b[39;49mpower(x, deg)]\n\u001b[1;32m    195\u001b[0m \u001b[39mreturn\u001b[39;00m poly\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/project1-grading/lib/python3.9/site-packages/numpy/lib/index_tricks.py:412\u001b[0m, in \u001b[0;36mAxisConcatenator.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m scalars:\n\u001b[1;32m    410\u001b[0m         objs[k] \u001b[39m=\u001b[39m objs[k]\u001b[39m.\u001b[39mastype(final_dtype)\n\u001b[0;32m--> 412\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconcatenate(\u001b[39mtuple\u001b[39;49m(objs), axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m    414\u001b[0m \u001b[39mif\u001b[39;00m matrix:\n\u001b[1;32m    415\u001b[0m     oldndim \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mndim\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambda_ = 0.0001\n",
    "degree = 25\n",
    "## tx is cleaned data \n",
    "poly = build_poly(x_train_filtered,degree)\n",
    "w , loss = ridge_regression(y_train,poly,lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean squared errors gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.00001\n",
    "max_iters = 2\n",
    "w_initial = np.zeros(cleaned_x_train.shape[1])\n",
    "\n",
    "w, loss = mean_squared_error_gd(y_train, cleaned_x_train, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean squared error sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.00001\n",
    "max_iters = 100\n",
    "w_initial = np.zeros(cleaned_x_train.shape[1])\n",
    "\n",
    "w, loss = mean_squared_error_sgd(y_train, cleaned_x_train, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = least_squares(y_train, cleaned_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.29695599e+01 2.51145669e+01 3.26484113e+01 3.16032914e+01\n",
      " 4.05931422e+01 6.39574453e+02 7.05774129e+01 9.55269539e+01\n",
      " 7.35393282e+01 3.62905491e+04 3.00867864e+01 4.64124920e+01\n",
      " 2.09206344e+04 5.64303677e+01 3.70916489e+01 2.53904536e+02\n",
      " 5.19182016e+01 2.52404526e+01 3.86315896e+01 4.45972177e+01\n",
      " 6.62916371e+01 3.37707816e+01 4.18467877e+01 8.99539567e+01\n",
      " 2.73960169e+01 4.63646235e+01 4.37618949e+01 4.65296156e+01]\n",
      "635309514315469.8\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(loss)\n",
    "print(len(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up x test\n",
    "\n",
    "cleaned_x_test = clean_data(labels, label_list, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109379, 29)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_test = build_poly(cleaned_x_test,degree)\n",
    "poly_test.shape\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes for next one working on this lol\n",
    "\n",
    "ridge -> build poly -> predict label: Ca marche mais pas très bons résultats (mieux si tu montes le degré mais tres bas F1)\n",
    "\n",
    "least squares -> predict label : marche pas avec build poly pck pas les bonnes dims (build poly rajoute une colonne, donc impossible de faire data@weight) Ca marche sans, mais on a que 1 valeur a 1, le reste a -1 (et ya pas de params a changer) F1 SCORE 0.000 ACCURACY 0.912\n",
    "\n",
    "mean squared gd -> (same thing pour build poly) les valeurs sont fucked up mdr [1.13497280e+79 1.72690226e+79 3.20689183e+78 ... 1.36781180e+78 1.99141883e+78 1.76606508e+78] c'est du 10^78 a peu pres, donc ya tout qui fini par etre a 1 et rien a -1 \n",
    "\n",
    "mean squared sgd -> same as gd mais on est plutot dans du 10^14, et que des negatifs\n",
    "\n",
    "J'ai aussi tenté ridge en filtrant les données d'une autre manière, aka juste prendre les colonnes qui ont plus que 70% de données (par rapport aux nan) et ca me donne des meilleurs résultats... snif snif donc F1 SCORE 0.139 ACCURACY 0.837 Après j'ai rien clean up or anything donc c'est un peu ridicule genre ya toutes les valeurs d'exceptions encore (en vrai j'ai aucune idée de pourquoi ca a marché ??? Pck j'ai pas enlevé les nan ?? Donc ca a du bader hahahaha)\n",
    "\n",
    "Si je mets 1000 max_iters ca overflow error, donc j'ai laissé 100, a tester avec 500 par exemple\n",
    "\n",
    "Idées: Peut être que normalizer les data ca change qqch mour les deux mean squares ? \n",
    "\n",
    "Questions TA:\n",
    "- regarder nos logisitc methods\n",
    "- pk ca marche pas lol\n",
    "- est - ce que c'est une bonne idée la manière dont on fait ou est-ce qu'il nous faut beaucoup plus de colonnes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noms des variables et valeurs utilisées\n",
    "\n",
    "- cleaned_x_train/test : data nettoyée avec nos 28 colonnes choisies\n",
    "- x_train/test_filtered : data avec enlevé ceux avec moins de 70% de donnees\n",
    "\n",
    "- degres: pour ridge utiliser un grand chiffre, pour le reste utiliser 1 (bah en fait c'est juste pour build poly donc pas utiliser (je crois, j'ai pas trouvé de valuers qui marche))\n",
    "\n",
    "- max_iters: j'ai l'impression que plus je mets grand, plus mes y_pred sont grands (why??????) (avec 1, j'ai que des -1.3 environ, quand je mets 2 j'ai entre 1 et 20, quand je mets 100 j'ai 10^78 lol) Ca explique le overflow error quand je mets 1000 i guess\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.03807512 18.15791472  4.53047456 ...  2.28483186  2.90662102\n",
      "  2.94509189]\n"
     ]
    }
   ],
   "source": [
    "yp = predict_labels(w,cleaned_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(np.where(yp == 1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids,yp,\"outputs/ridge_degre25_200features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
